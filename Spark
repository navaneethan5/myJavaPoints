Variable declaration: var myString:String="hello"
Function declaration: def funName(z:Int,y:Int):Int = {}
Array declaration: val x = Array("1","2","3"). Can be accessed by using x(0). Should be of same data type;Mutable;
List declaration: val y = List(1,2,3). Can be accesed by y(0). Should be of same type; Immutable;
Tuples: Is an ordered container similar to array with 2 or more values; Index starts with 1; eg val emp= {1,"type"}. Can be of different types. Can be accessed by using emp._1.
Map declaration: val map1 = Map("id" -> 1,"name" -> "test"}. Can be accessed by using map1("id")
For loop: for (i <- 1 to 20 if i % 2 == 0)
RDD: Immutable collection of simple objects. Can be split into multiple partitions which in turn can run individually on each node in cluster.
RDD creation: Can be created by loading an external dataset or by distributing a collection of objects to the driver program.
Types of RDD operations: Transformations, which in turn creates a new RDD /Actions, returns the computed results of RDD and can return to driver/hdfs.
Lazy evaluation: Spark creates an rdd only when an action is made, otherwise it will the metadata of RDD creation code.
Cache/Persist: Cache and persist both store data where cache stores at default memory storage but persist stores based on user inputs.
RDD creation examples: a) val lines = sc.parallelize(List(“hi”, “how are you”) b) val lines = sc.textFile(“README.md”)
Filter Transformation: val rdd1 = rdd.filter(x => x.contains(“error”))
Lineage graph: Spark keeps track a set of dependencies between different RDDs. Used to recover any lost data or to create RDD on demand.
Take action example: rdd.take(3).foreach(println)
Collect action:rdd.collect //the entire dataset should fit in a single machine. Has always to be associated with mkString(“ “)
Functions: When passing functions, the objects that are used in the function should be always serialized.
Map Transformation: Works on each element in RDD and returns the same number of elements, but the element datatype can be different from the other.
FlatMap Transformation: Similar to map but produces multiple elements for each input element.It returns RDD of elements in list rather than list of
Split example: rdd.flatMap(x => x.split(“ “))
Pseduo Transformation: distinct- expensive/union- cheap,contains duplicates/intersection - expensive/subtract - takes elements in rdd1 which is not present in rdd2.
Carteasn Transformation: returns rdd1 {x,y,z} , rdd2 {a,,b,c} as {(x,a), (x,b)… (z,c)}
Reduce Action: Takes a function that operates of 2 elements of that type in RDD; dd.reduce((x,y) => x+y)
Fold Action: Similar to reduce except it takes “zero value” to be the initial call for each partition.
Take Action: Returns the number of elements mentioned but will not return in the same order
Top Action: Used to take the top elements based on the ordering
Sample Action: Used to sample data . rdd,takeSample(false,1) ; If true is mentioned it returns the number of elements mentioned in the count with duplicating values of rdd.
Count Action:Returns count
Count By Value Action: returns no of times each elements occurs in RDD
Take Ordered Action: returns the rdd in the natural sorting order similar to take.
Foreach Action : rdd.foreach(println)
Implicit conversions: Scala is able to do implicit conversions similar to java for RDD.
Persistence: Scala stores the data in JVM heap space as unserialized. If stored in disk , it will be always serialized. e.g. result.persist(StorageLevel.DISK_ONLY); result.unpersist()
Persistence levels: MEMORY_ONLY - more space used, less CPU time/ MEMORY_ONLY_SER - less space used, more CPU time/MEMORY_AND_DISK - more space used, medium CPU time/MEMORY_AND_DISK_SER - low space used, high cpu time/DISK_ONLY - low space used, high cpu time
Pair Value RDD: Used to count reviews for each product, grouping together data with same key, grouping together different RDDs
Pair Value RDD Creation: val pairs = lines.map(x => (x.split(" ")(0), x))
ReduceByKey Transformation: rdd.reduceByKey((x, y) => x + y) // adds values grouped by key; Perform reduce, but by key
GroupByKey Transformation: rdd.groupByKey() //groups values based on the key . Perform aggregation like sum , average by key
MapValues Transformation: rdd,mapValues(x => x+1) //apply function to each values without changing key
FlatMapValues Transformation: rdd.flatMapValues(x => x.to(5)) //apply function to each values and generate key value pair with old rdd 
Keys Transformation: Returns a list of keys //rdd.keys
Values Transformation: Returns list of values //rdd.values
SortByKey Transformation: Returns results in sorted order //rdd.sortByKey
SubtractByKey Transformation: Returns the rdd1 key value which is not in rd.
Join Transformation: Groups by key with rdd1 and rdd2 ;val joined = userData.join(events)// RDD of (UserID, (UserInfo, LinkInfo)) pairs
RightOuterJoin Transformation: Perform join but the key should be present in rdd1 {(3,(Some(4),9)),(3,(Some(6),9))}
LeftOuterJoin Transformation: Perform join but the key should be present in rdd1.{(1,(2,None)), (3,(4,Some(9))), (3,(6,Some(9)))}
Cogroup Transformation: Groups the data from both RDDs sharing the same key .eg {(1,[2].[]) , (3, [4,6], [9])}
Filter Action: rdd1.filter{case (key, value) => value >3}
PerKeyAverage: rdd.mapValues(x => (x, 1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))
Word Count: val result = words.map(x => (x, 1)).reduceByKey((x, y) => x + y)
To check partitions size: result.partitions.size ; To explicitly partition, provide the partition parameter as a number in rdd actions.
Repartition: Repartition can be done by using repartition or coalesce . Repartition is expensive as it shuffles entire date. Coalese is optimized but can be used only for reducing rdds 
CountByKey: counts the number of keys in pair rdd; produces a count by each key in RDD
CollectAsMap: collects the results as map format. If duplicate keys are there , it is getting override
Lookuo: returns the value for the lookup key
Partitions in key value pairs: Keys can be grouped in 1 node when partitions are defined, but cannot control which key goes to which node.
Partition method: val userdata = sc.sequenceFile[UserId, UserInfo]("hdfs://...").partitionBy(new HashPartitioner(100)).persist().
Persist partition: Always persist the partitiondata else it will reshuffle the entire partition again.
HashPartition: val partioned = pairs.partitionBy(new org.apache.spark.HashPartitioner(2));To check, partioned.partitioner
Benefits from partioning: cogroup(), groupWith(), join(), leftOuterJoin(), rightOuter,Join(), groupByKey(), reduceByKey(), combineByKey(), and lookup()
Supported file formats: text, json,csv, sequencefiles, protocol format, object files (o/p format to hadoop will always be a key value pair)
Whole file: val input = sc.wholeTextFiles("file://home/holden/salesFiles") ; creates key value pair, where key is the file name
Save text files: pair.saveAsTextFile("/tmp/test") ; this saves text files in op directory based on the partion size alomg with success file
Hadoop Ditributed file system: Distributes these files across the nodes within a cluster.Fault tolerance - always stores 3 copies. Files only be created/deleted, cannot be updated. Data replication.Code -> data
HDFS architecture: One "Name node" per cluster contains metadata about the cluster.Data node acts as slave.
HDFS storing data: Files split as data blocks. Each block is replicated. Clients asks Namenode to write where. Name node gives a list of data nodes and client writes directly to data node.
Sqoop: Its a tool to transfer rdbms data to hdfs
Spark management: Yarn, mesos, standalone
Spark storage: local, hdfs, s3, nosql, rdbms
Spark architecture: master node -> cluster manager -> task node
Spark context: spark object, used to create rdd, works with cluster manager, splits tasks across nodes, partitions across nodes, results returned to driver program.
Aggregate: RDD = [3,5,4,7,4] => RDD1 = [3,5,4] , RDD2 = [7,4] ; SeqOp: (lambda x, y : x[0]+y, x[1]*y) ;combOp (lambda x,y : x[0]+y[0] , x[1]*y[1]) ; collData.aggregate((0,1), seqOp, combOp); return [(12,60), (11,28)] => [23, 1650]
AggregareByKe Action: Perform aggregate by key.
Storing RDDs: Use language specific libraries for persistence instead of spark utilities.
Paraellism paramter: spark.default.parallelism - default is the number of cores available in the cluster.
Broadcast variable: Read only variables that is shared by all nodes.Used for lookup tables abd similar functions.val sedanTest = sc.broadcast("TMAX")
Accumulators: Global variable that is local to every node in the cluster.Shared variable that can be updated by all nodes.eg.  val count2 = sc.accumulator(0)
To print in format: for(x <- rdd.collect){println(x)}
Shortest line in RDD: rdd.reduce((x,y) => if(x.length < y.length) x else y); 
Aggregate eg : collData.aggregate(0)((x,y) => x+y, (x,y) => x+y) same ad collData.reduce((x,y) => x+y)
Partitions: collData.getNumPartitions() - derived by number of cores in machine; can be controlled by val rdd = sc.parallelize(Array(1,2,3,4)),2);
Dataframe: Is an rdd with orgaized rows and columns. has a schema - column names and data types
Dataframe supported operations: Filter, Join, GroupBy, Agg, RegisterAsTempTable
Sqlcontext: Similar to spark context.val sqlContext = new org.apache.spark.sql.SQLContext(sc) ; import sqlContext.implicit._;
Create Dataframe: val drdd = sqlContext.read.json("json file") ;drdd.show() - similar to select *; drdd.printSchema() - display the schema
Select query: drdd.select("<column name>").show
Filter query; zip.filter(zip("city") === "CHICOPEE").show
GroupBy query:zip.groupBy("city").count().show()
Join: emp.join(dept,emp("depid") === dept("deptid")).show
Temp table: zip.registerTempTable("zip1");sqlContext.sql("select * from zip1 where city = 'CHICOPEE'").show
JDBC: val demoOf = sqlContext.read.format("jdbc").options(Map("url" -> "jdbc:mysql://localhost:3986/demo" , "driver" -> "com.mysql.jdbc.Driver", "dbtable" -> "demotable", "user" -> "root", "password" -> "")).load
Create dataframes from RDD:val autoDF = rowRDD.toDF("name","gender","age") ; RDD should be a tuple
DStream: Discretized Stream. Data is receieved in DStream and collected as a microbatch. Each microbatch is a RDD.
Windowing/Sling: Functions are available in DStream to filter the number of readings.
StreamingContext: Import import org.apache.spark.streaming.StreamingContext._ and import org.apache.spark.streaming.Seconds
Declaration of Streaming Context: val ssc = new StreamingContext(sc, Seconds(1))'
Spark MLIB: Comes with 2 flavours - spark.mllib, spark.ml
Special data types: Local vector and labelled point. Any mlib related processing needs to be converted to these data types before processing.
Textfile to dataframe: val df = textFile.toDF("line")
Map return type: It always returns array of strings
https://gist.github.com/tmcgrath
