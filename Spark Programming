SparkContext: 
  Object that tells spark how to access the cluster. It can be used to create RDD, accumulators and broadcast
  variables that can be used across the cluster. Only one SparkContext can be created per JVM(need to stop 1 to create another)

SparkContext syntax:
  SparkContext sc = new SparkContext(new SparkConf().setMaster(master).setAppName(myapp)) where myapp is the app name and master
  is Spark Mesos , Yarn cluster url or "local" when running in stand alone mode.

Using the shell:
  To specify local cluster running on 4 cores, spark-shell --master local[4]
  To execute a code , spark-shell --master local[4] --jar sample.jar
  To include a dependency, spark-shell --master local[40 --packages "org.example:example:01."

RDD:
  Resilent Distributed Dataset is a fault tolerant collection of elements that can be executed in parallel by
  paritioning across nodes.

RDD creation:
  Can be created by either parallelizing or loading from external datasets
  Parallelizing, val data = sc.parallelize(Array [1,2,3])
  External data, val data = sc.textFile("sample.txt")

RDD partitions:
  Spark creates 1 partition per file block (typically 64MB for hadoop). It cannot be repartioned with less no of partitions
  when compared to blocks.

Read a directory:
  Use SparkContext.wholeTextFiles to read a directory and returns pair RDD (filename, content)

To save a file:
  Use SparkContext.objectFile or rdd.saveAsObjectFile , stores as serialized objects

RDD Operations:
  Transformations , results in a new dataset by operating each element in the original dataset
  Actions, results in returning the results to the driver program

Persistence:
  RDD is recomputed everytime when an action is applied. Can be persisted by using persist/cache option. Use rdd.unpersist
  to remove persistence
  
Lazy evaluation:
  Results are computed only when actions are applied and not immediately when transformation is applied.

Function definition:
  Function can be defined by, class MyClass { def func1(s: String): String = { ... } }

Closures:
  Spark breaks up operation into tasks and sends each task to the executor. Prior to execution, spark evaluates task closures
  ie the variables/methods that are visible to each executors. It serializes and send to the executors. The variables to the
  executors are copies of the original operation and does not mutate the value in the driver program. Use accumulators to 
  avoid non mutation

Print elements:
  rdd.collect.foreach(println) - collects the executor results to driver program
  rdd.take(n).foreach(println) - takes n elements and prints
  
Key-Value pairs:
  Special type of RDD represented with key and values. Additional operations like grouping/aggregating/sorting the elements
  by key.

Transformation types:
  RDD transformations - map, filter, flatMap (operates on each element and returns 0 to more elements), union, intersection,
  distinct.
  Pair RDD transformations - groupByKey, reduceByKey, aggregateByKey, sortByKey, join, cartesian, 

Action types:
  RDD actions - reduce, collect, count, first, take, takeOrdered(n [ordering]), foreach
  Pair RDD actions - countByKey

Shuffle:
  When data across the paritions needs to be aggregated ie for eg reduceByKey needs multiple partition data to be shuffled and
  grouped in 1 partition.Shuffle creates lots of intermediate files on the disk(map and reduce). The temp storage dir can be
  specified using spark.local.dir config parameter.

Types of persistence:
  MEMORY_ONLY - Store rdd in memory. If does not fit, some partitions will not be cached and calculaed on the fly
  MEMORY_AND_DISK - Same as memory only but also stores in disk when does not fit in
  MEMORY_ONLY_SER - Stores as serialized, memory efficient but more CPU intensive
  MEMORY_AND_DISK_SER - Simliar to memory only ser, but also stores in disk when does not fit in
  DISK_ONLY - Stores partitions on the disk
  MEMORY_ONLY_2 .. - Same as above but replicate partition on each cluster node

Broadcast variables:
  Read only variable cached on each machine rather than sending a copy of the variable along with many tasks.Once broadcasted
  should not be changed. Can be invoked by val bVariable = sc.broadcast(Array(1,2,3)). Can be accessed by bVariable.value

Accumulators:
  Shared variables that are used to update the values like counter. The tasks can update the value but cannot read. Only 
  driver program can read the value. val accum = sc.accumulator(0, "My Accumulator"). Can be applied to only action and every
  tasks updates the accumulator only once ie restarted tasks will not update the accumulator
